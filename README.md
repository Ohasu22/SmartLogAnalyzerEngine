# Distributed Log Analyzer Engine

**Author:** Ojas Gharde  
**Domain:** Backend Systems · Distributed Systems · Stream Processing · Observability  
**Tech:** Python, Generators, Deques, Hash Maps, Statistical Analysis

---

## TL;DR (For Recruiters)

- Built a scalable streaming log analysis engine inspired by production observability systems  
- Processes millions of logs in O(n) time and O(1) space using generator-based pipelines  
- Implements error spike detection, rolling statistical anomaly detection, and temporal pattern matching  
- Designed with modular architecture and configuration-driven parameters  

---

## Overview

The **Distributed Log Analyzer Engine** is a Python-based backend system designed to process, analyze, and correlate high-volume logs generated by distributed services.

The project emphasizes:
- Streaming data processing
- Memory-efficient design
- Time-bounded analysis
- Statistical anomaly detection
- Maintainable and extensible architecture

This implementation intentionally avoids heavy frameworks to demonstrate core backend and systems-level problem solving.

---

## High-Level Architecture

![System Architecture](assets/system_architecture.png)

Pipeline overview:


---

## System Pipeline (Detailed)

![System Pipeline](assets/system_pipeline.png)

Each stage operates independently and communicates using immutable data structures, allowing easy extension or replacement of components.

---

## Log Design

Each log entry is normalized into an immutable tuple:


**Why tuples?**
- Prevent mutation during pipeline traversal
- Safe data transport between modules
- Low overhead and fast access

---

## Key Design Decisions

### 1. Generator-Based Streaming

Initial file-based parsing was replaced due to scalability limitations.

![Generator Streaming](assets/generator_streaming.png)

**Benefits:**
- Space Complexity: O(1)
- Time Complexity: O(n)
- Supports real-time ingestion
- Handles millions of logs efficiently

---

### 2. Frequency Analysis

![Frequency Analysis](assets/frequency_analysis.png)

- Uses `defaultdict`
- Tracks service-level and severity-level distributions

**Complexity:**
- Time: O(1) per log
- Space: O(k) where k = number of unique services and levels

---

### 3. Error Spike Detection

![Error Spike Detection](assets/error_spike_detection.png)

Detects bursts of errors within a sliding time window using `deque`.

- Threshold-based spike detection
- Time-bounded analysis
- Constant-time updates

---

### 4. Rolling Statistical Anomaly Detection

![Rolling Statistics](assets/rolling_statistics.png)

Implements rolling mean and standard deviation without recomputation.

- Maintains rolling sum and sum of squares
- Detects anomalies dynamically
- Reduces false positives compared to static thresholds

---

### 5. Temporal Pattern Detection

![Pattern Matching](assets/pattern_matching.png)

Detects ordered event sequences such as:


Constraints:
- Order-sensitive
- Time-gap bounded
- Stream-safe (no regex)

---

## Configuration-Driven Design

![Config Driven Design](assets/config_design.png)

All parameters (thresholds, windows, patterns) are defined in `config.py`.

Benefits:
- No hardcoded values
- Easy tuning
- Environment-specific configuration
- Cleaner pipeline logic

---

## Output Metrics

![Final Metrics](assets/final_metrics.png)

At the end of execution, the engine reports:
- Total logs processed
- Total error windows analyzed
- Top services by log volume
- Detected patterns with counts
- Anomaly events

---

## Integration Guide

1. Clone the repository
2. Choose input mode:
   - File-based logs → enable `main()`
   - Streaming logs → use `stream_main()` (default)
3. Replace the generator with your real log source if needed
4. Customize patterns and thresholds in `config.py`
5. Run the engine

Reusable modules:
- `analysis/pattern_matcher.py` – temporal correlation
- `anomaly/statistical.py` – anomaly detection
- Generator pipeline – scalable ingestion

---

## Engineering Challenges Addressed

| Challenge | Solution |
|---------|----------|
| Memory explosion | Generator-based streaming |
| Error bursts | Sliding window with deque |
| False positives | Rolling statistical thresholds |
| Temporal correlation | Time-bounded pattern matching |
| Maintainability | Modular, config-driven design |

---

## Future Enhancements

- Kafka / PubSub ingestion
- JSON-based structured logs
- Persistent metric storage
- Visualization dashboards
- Real-time alerting hooks

---

## Why This Project Matters

This project demonstrates:
- Backend system design principles
- Stream processing fundamentals
- Observability and monitoring concepts
- Performance-conscious Python engineering

It mirrors real production tradeoffs found in distributed systems rather than being a purely academic implementation.
