DISTRIBUTE LOG ANALYZER ENGINE
-Ojas Gharde
-ojasgharde22@gmail.com
-Github Profile




Hello! I am Ojas and I am currently a college student in Electronics and Telecommunications Engineering with a strong passion in System-based Software Development. This project really took me around 1-2 months from its idea conception to its refinement. I have tried to make this as modular as possible so anyone could easily integrate my project into theirs for their log insights.
I am still working on the project and improving it ever so slightly, whenever I get the chance so I would love to have any second opinions on improving the project to truly make it a production level project! 
Contents
Overview	3
Goals/ Objectives	3
Features	3
System Architecture	3
Phase 1 – Log Parsing	7
My Approach:	7
Design	9
Phase 2 – Redesigning log parsing using Streaming via Generators	10
Phase 3 – Frequency Analysis	11
Phase 4 – Error Spike Detection	11
Phase 4: Part 2- MEAN and STD Calculation	12
Phase 5- Temporal Pattern Detection	14
Configuration-Driven Design	15
Final metrics and Summary Output	15
How to integrate My Project into Yours	16
Engineering Challenges and Solutions	17
Challenge: Memory explosion	17
Challenge: False Positives for anomaly alerts	17
Challenge: Temporal correlation	17
Challenge: Maintainability and Scalability	17
Future Improvements	18
Closing Note	18

 
Overview
The Distributed Log Analyzer Engine is a python based system which I designed to parse, analyze and correlate huge amounts of log data across distributed environments. The system helps the enterprise identify patterns, anomalies, and provides various insights like frequency analysis from large volumes of logs generated by multiple services or nodes.

Goals/ Objectives
1.	Achieve O(n) time complexity in parsing of logs from distributed sources
2.	Implement a pattern matching algorithm and anomaly detection
3.	Create a scalable project
4.	It should be able to integrate with any systems with minor tweaks
5.	Write a clean code

Features
1.	Log parsing using hash maps instead of regular-expression(regex) parsing
2.	Pattern matcher with anomaly detection
3.	Statistical analysis like frequency calculation(custom rule based)
4.	Linear pipeline

System Architecture
My system follows a linear but modular pipeline architecture. First the logs are generated using generator. Generator is also unique in itself but let’s talk about it later. Logs are generated in my unique standard[1]. As the generator generates the logs, log parser is tasked to parse them using hashmaps and segregate them into maps. Parsed logs are fed into my pattern matcher for analysis and anomaly detection. If a pattern is matched then it returns “PATTERN DETECTED”.
If my timestamp yields an ERROR in a timeframe of window_seconds, then the system increments the error counter by 1. Instead the analysis goes to Rolling mean algorithm to detect any anomalies in the window. Rolling mean algorithm calculates mean, standard deviation and is_anomaly parameter and returns True if is_anomaly is True.
Lastly all this combinates into the final frequency and log analysis of the log streams a the last.
 
 
System Pipeline

 
System Architecture


 
Anomaly detected – 1M test Run

 
Frequency analysis-1M test Run

 
Phase 1 – Log Parsing
My Approach:
Let’s assume the logs are stored in a file called sample.log, so whenever the log_parser.py is called, the code read logs line by line from that static file.
First, I needed a standard log design. After some research I found that Google uses a custom JSON format which is like this:
{
  "logName": string,
  "resource": {
    object (MonitoredResource)
  },
  "timestamp": string,
  "receiveTimestamp": string,
  "severity": enum (LogSeverity),
  "insertId": string,
  "httpRequest": {
    object (HttpRequest)
  },
  "labels": {
    string: string,
    ...
  },
  "metadata": {
    object (MonitoredResourceMetadata)
  },
  "operation": {
    object (LogEntryOperation)
  },
  "trace": string,
  "spanId": string,
  "traceSampled": boolean,
  "sourceLocation": {
    object (LogEntrySourceLocation)
  },
  "split": {
    object (LogSplit)
  },
  "errorGroups": [
    {
      object (LogErrorGroup)
    }
  ],
  "apphub": {
    object (AppHub)
  },
  "apphubDestination": {
    object (AppHub)
  },
  "apphubSource": {
    object (AppHub)
  },

  // Union field payload can be only one of the following:
  "protoPayload": {
    "@type": string,
    field1: ...,
    ...
  },
  "textPayload": string,
  "jsonPayload": {
    object
  }
  // End of list of possible types for union field payload.
}

Google Cloud LogEntry


I decided to design my own custom Log Entry. My log line consists of 4 parsable entities:
•	Timestamp: the timestamp of the log when it was created
•	Level: there are three levels: INFO, WARN, ERROR indicate the level of severity of the log
•	Service: which service produced the log: AuthService, PaymentService, Shaktiiiii
•	Message: extra entity where it could be anything depending on the log or company, I took it as the userID.
 
Sample log


Design
These logs will be parsed using split() and strip() functions to create a list where
List = [timestamp, level, service, message]

After thinking about it, using list is not the most optimal and secure solution as list is mutable and hackers could change or meddle with the list.
To combat this I have decided to use a tuple instead of a list to send the data to other functions. Now the tuple will be where,
Tuple  = (timestamp, level, service, message)
This way once a tuple is created it wont change while in transit in packets to other functions.
Drawbacks Discovered:
•	Reading whole file again and again just to read one line if there are multiple logs in the file
•	Not Scalable for large logs
•	No real time detection
 
Frequency analysis of file based log(before redesigning)

 
Phase 2 – Redesigning log parsing using Streaming via Generators
File based system is very memory heavy especially for huge amount of logs, it certainly wouldn’t be able to handle the load of constant incoming logs and whenever a new log comes into the file, the parser has to traverse through the whole file again from the top to reach that new log which makes the time complexity as O(n!) each time a new log is added.
I absolutely have to change this bottleneck, a generator based log can solve this problem which can mimic the real working of logs, it streams the logs one by one through the function
generate_log_stream(num_logs)
num_logs = the number of logs we want to generate
This significantly improves the performance, the space complexity now is O(1) and at a time only one tuple is existing for the analysis time complexity is O(n) for the generate_log_stream function where n = num_logs

Python concept I used:
•	Generator doesn’t return, it yield so gives the output one at a time
Now I can make millions of logs instantly.
 
Yield function in generator.py line 29

Note: strftime and strptime are two different functions, I had accidently used strptime in this line and had gotten error, Nice to learn something new!
•	Strptime : used to convert a string into a datetime object
•	Strftime: used to convert datetime object into a string
Keep in mind next!
 
Phase 3 – Frequency Analysis
Tuples are coming through the generator.py in the format:
(timestamp, service, level, message)
Simple could use the default dictionary to count the service and level:
 
Frequency incrementor keeping service and level as keys

Time complexity is O(1) for each tuple line.
If there are n = num_logs then time complexity becomes O(n)
Space complexity is O(k) as there us a use of dictionary and depending on the number of service and levels the value of k changes

Phase 4 – Error Spike Detection
If multiple same errors are popping up in the logs, then there has to be a spike detection for that to be detected.
There are multiple ways to detect this but because I want to do this in O(1) time I only looked for the methods which has the best time complexity
I have to implement anomaly detection in here as well. After researching for some anomaly detection techniques, 
Mean Absolute Deviation – little advanced 
EWMA Control charts – Industry Standard( little slow for my project)
Most simple one is Rolling mean with standard deviation – decided to implement this
I did this in two parts, first the analysis/spike_detector.py which implements the sliding window with a deque to detect spikes in a specific time window
Detect_error_spikes has three parameters:
•	Parsed_logs : The tuple lines coming from the log_parser.py
•	Threshold: a limit if crossed triggers the spike
•	Window_seconds: the timeframe in which if the same error occurs more than the threshold it triggers the spike(sliding window)

The spike_detector.py returns a list of tuple of (timestamp, len(error_timestamps)) where error_timestamps : a deque of timestamps within the decided window

Phase 4: Part 2- MEAN and STD Calculation
Anomaly/statistical.py does the mean and STD calculation
This also uses the sliding window technique so that the calculation takes a time complexity of O(1).
The variables used are:
 
Initiator values for RollingStats

Window_size is the same as window_seconds[2]
Threshold: a value to be multiplied by std for comparison
Sum: for calculation
Sum_square: for calculation

Library named Math is imported to do basic math calculations.
Update function:
•	Appends the values in the deque named self.values
•	Increments the value into sum
•	Sum_square multiplies the values by itself and stores the new value
•	After this, if the length of the deque is bigger that the window_size then we pop the deque from left, store that value into the local variable old and subtract the old from the sum and sum_square accordingly
•	Because the values are coming in a stream, it is not required to use for loop, a simple linear calculation is fine




Mean function:
As simple as its name, calculates the mean of the values
 
Mean calculator

Std function:
 
Standard Deviation calculation

Is_anomaly function:
Returns true or false depending upon the comparison of the value and the sum of mean and standard deviation
 
Is anomaly function

Time complexity: O(1)
Space complexity: O(n) even though a deque is used, there are only n=values depending on window size is being occupied so it could become O(1)  
Phase 5- Temporal Pattern Detection
The program is designed to detect a specific pattern in the logs within a limited time gap.
Implemented in analysis/pattern_matcher.py
Patten_matcher.py requires to parameters
•	A list of tuple named pattern to find
•	Maximum time gap acceptable if the pattern is detected
Using two deques for the window and timestamps and a dictionary pattern_count to store the count

Process function(before redesign):
Only uses the service and not uses level to identify pattern
If the window is equal to the pattern then increment a value called matches by 1 which I was thinking of using in the end of the analysis chart to show how many matches are found for that specific pattern
 
Process function

 
Analysis function(redesigned process function):
Uses both service and level parameters for accurate pattern detection
(service,level) tuples
Max_time_gap ensures that the pattern is meaningful:
 
Max_time_gap logic

Could have used regex but its expensive and very difficult to implement on streams.

Configuration-Driven Design
To avoid hardcoding the values in the parameters of each and every node of the pipeline, all parameters are defined in config.py.
 
Config.py

Final metrics and Summary Output
At the end of execution, me engine reports:
•	Total Logs processed
•	Total error windows analysed
•	Top services by log volume
•	Detected patterns and counts
 
Frequency analysis-1M test Run

How to integrate My Project into Yours
•	Clone the repository
•	There are two ways you can divert the pipeline, if your logs are getting stored in a file like sample.log then enable the main function(which is in main.py and comment out the stream_main) but if you want to have a stream output then don’t change anything, it is already configured for stream input.
•	Right now the generator is being used to create the logs in stream_main function inside main.py, to integrate your log output to the input of the stream_main, comment out these lines
 
Lines to comment

•	Integrate the pattern as to your liking, no need to change the num_logs parameter as I have done error handling already.
•	Run the stream engine(you can ignore the above two instructions and run it as it is pulled to get the output for the generated logs)
 
Other Integrations you can do:
patternFinder in analysis/pattern_matcher.py for temporal event detection
RollingStats in anomaly/statistical.py for anomaly detection
Generator-based pipeline for scalable processing

Simply feed your own log source into stream_main().

Engineering Challenges and Solutions
Challenge: Memory explosion
Use generator based streaming + deques and sliding windows
Challenge: False Positives for anomaly alerts
Rolling statistics instead of fixed thresholds
Challenge: Temporal correlation
Time bounded pattern matching
Challenge: Maintainability and Scalability
Modular architecture and config driven design
 
Future Improvements
•	Kafka and PubSub integration
•	JSON log output
•	Persistent metric storage
•	Visualization dashboards

Closing Note
This project demonstrates system thinking, stream processing, and real-world monitoring design. It is intentionally designed to mirror the constraints and tradeoffs faced in production-scale observability systems.
