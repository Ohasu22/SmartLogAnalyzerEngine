# Distributed Log Analyzer Engine

**Author:** Ojas Gharde 
**Domain:** Backend Systems · Distributed Systems · Stream Processing · Observability  
**Tech:** Python, Generators, Deques, Hash Maps, Statistical Analysis

---

## TL;DR (For Recruiters)

- Built a scalable streaming log analysis engine inspired by production observability systems  
- Processes millions of logs in O(n) time and O(1) space using generator-based pipelines  
- Implements error spike detection, rolling statistical anomaly detection, and temporal pattern matching  
- Designed with modular architecture and configuration-driven parameters  

---
Live Deployment

The engine is publicly deployed and accessible via Render.

Note
The service runs on Render’s free tier and may take ~30–60 seconds to wake up after inactivity (cold start behavior).

Base URL:

https://smartloganalyzerengine.onrender.com

##Health Check

Endpoint:
GET /

Example:
https://smartloganalyzerengine.onrender.com/

Returns:
<p align="center">
  <img src="smartLog/assets/status.png" width="700"/>
</p>

##Run Log Analysis

Endpoint:
GET /run

Example:

https://smartloganalyzerengine.onrender.com/run

To determine the number of logs to generate, define the variable num_logs

Example:

https://smartloganalyzerengine.onrender.com/run?num_logs=1000

Returns structured JSON including:
Total logs processed
Total error windows analyzed
Service-level frequency distribution
Severity-level distribution
Detected patterns

Example response:
<p align="center">
  <img src="smartLog/assets/response.png" width="700"/>
</p>

Benchmark response:
<p align="center">
  <img src="smartLog/assets/1Mresponse.png" width="700"/>
</p>

It takes a little time to run so be patient!

##Interactive API Documentation (Swagger UI)

Explore and test endpoints directly in your browser:

https://smartloganalyzerengine.onrender.com/docs

Features:
Execute endpoints interactively
Inspect request/response schemas
Validate output structures

## Overview

The **Distributed Log Analyzer Engine** is a Python-based backend system designed to process, analyze, and correlate high-volume logs generated by distributed services.

The project emphasizes:
- Streaming data processing
- Memory-efficient design
- Time-bounded analysis
- Statistical anomaly detection
- Maintainable and extensible architecture

This implementation intentionally avoids heavy frameworks to demonstrate core backend and systems-level problem solving.

---

## High-Level Architecture

<p align="center">
  <img src="smartLog/assets/system_architecture.png" width="700"/>
</p>

Pipeline overview:


---


## System Pipeline (Detailed)

<p align="center">
  <img src="smartLog/assets/log_pipeline.png" width="700"/>
</p>

Each stage operates independently and communicates using immutable data structures, allowing easy extension or replacement of components.

---

## Log Design

Each log entry is normalized into an immutable tuple:


**Why tuples?**
- Prevent mutation during pipeline traversal
- Safe data transport between modules
- Low overhead and fast access

---

## Key Design Decisions

### 1. Generator-Based Streaming

Initial file-based parsing was replaced due to scalability limitations.


**Benefits:**
- Space Complexity: O(1)
- Time Complexity: O(n)
- Supports real-time ingestion
- Handles millions of logs efficiently

---

### 2. Frequency Analysis


- Uses `defaultdict`
- Tracks service-level and severity-level distributions

**Complexity:**
- Time: O(1) per log
- Space: O(k) where k = number of unique services and levels

---

### 3. Error Spike Detection


Detects bursts of errors within a sliding time window using `deque`.

- Threshold-based spike detection
- Time-bounded analysis
- Constant-time updates

---

### 4. Rolling Statistical Anomaly Detection



Implements rolling mean and standard deviation without recomputation.

- Maintains rolling sum and sum of squares
- Detects anomalies dynamically
- Reduces false positives compared to static thresholds

---

### 5. Temporal Pattern Detection


Detects ordered event sequences such as:


Constraints:
- Order-sensitive
- Time-gap bounded
- Stream-safe (no regex)

---

## Configuration-Driven Design



All parameters (thresholds, windows, patterns) are defined in `config.py`.

Benefits:
- No hardcoded values
- Easy tuning
- Environment-specific configuration
- Cleaner pipeline logic

---

## Output Metrics



At the end of execution, the engine reports:
- Total logs processed
- Total error windows analyzed
- Top services by log volume
- Detected patterns with counts
- Anomaly events

---

## Integration Guide

1. Clone the repository
2. Choose input mode:
   - File-based logs → enable `main()`
   - Streaming logs → use `stream_main()` (default)
3. Replace the generator with your real log source if needed
4. Customize patterns and thresholds in `config.py`
5. Run the engine

Reusable modules:
- `analysis/pattern_matcher.py` – temporal correlation
- `anomaly/statistical.py` – anomaly detection
- Generator pipeline – scalable ingestion

---

## Engineering Challenges Addressed

| Challenge | Solution |
|---------|----------|
| Memory explosion | Generator-based streaming |
| Error bursts | Sliding window with deque |
| False positives | Rolling statistical thresholds |
| Temporal correlation | Time-bounded pattern matching |
| Maintainability | Modular, config-driven design |

---

## Future Enhancements

- Kafka / PubSub ingestion
- JSON-based structured logs
- Persistent metric storage
- Visualization dashboards
- Real-time alerting hooks

---

## Why This Project Matters

This project demonstrates:
- Backend system design principles
- Stream processing fundamentals
- Observability and monitoring concepts
- Performance-conscious Python engineering

It mirrors real production tradeoffs found in distributed systems rather than being a purely academic implementation.
